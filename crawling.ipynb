{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offline_indexer.py\n",
    "\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urldefrag, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect_langs\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import deque, defaultdict\n",
    "import pickle\n",
    "\n",
    "# Ensure stopwords are available\n",
    "import nltk\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineCrawler:\n",
    "    \"\"\"Manually triggered crawler: from seed URLs, collect page texts.\"\"\"\n",
    "    def __init__(self, seeds, max_depth=2, delay=0.5):\n",
    "        self.seeds = seeds\n",
    "        self.max_depth = max_depth\n",
    "        self.delay = delay\n",
    "\n",
    "    def run(self):\n",
    "        frontier = deque([(url, 0) for url in self.seeds])\n",
    "        visited = set()\n",
    "        pages = []  # list of (url, raw_text)\n",
    "\n",
    "        while frontier:\n",
    "            url, depth = frontier.popleft()\n",
    "            if url in visited or depth > self.max_depth:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=5)\n",
    "                resp.raise_for_status()\n",
    "                html = resp.text\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] fetching {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Extract visible text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for tag in soup([\"script\", \"style\"]):\n",
    "                tag.decompose()\n",
    "            text = soup.get_text(separator=\" \")\n",
    "            print(text[:1000], \"...\")  # Print first 100 chars for debugging\n",
    "\n",
    "            # Language filter\n",
    "            langs = []\n",
    "            try:\n",
    "                langs = detect_langs(text)\n",
    "            except:\n",
    "                pass\n",
    "            if not any(l.lang == \"en\" and l.prob >= 0.9 for l in langs):\n",
    "                print(f\"[SKIP] non-English: {url}\")\n",
    "                continue\n",
    "\n",
    "            pages.append((url, text))\n",
    "\n",
    "            # enqueue links if depth allows\n",
    "            if depth < self.max_depth:\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    href = urljoin(url, a[\"href\"])\n",
    "                    href, _ = urldefrag(href)\n",
    "                    p = urlparse(href)\n",
    "                    if p.scheme in (\"http\", \"https\"):\n",
    "                        frontier.append((href, depth + 1))\n",
    "\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "        return pages\n",
    "\n",
    "def postings_dict():\n",
    "    \"\"\"Creates a default dictionary for postings.\"\"\"\n",
    "    return defaultdict(int)\n",
    "\n",
    "class StemIndex:\n",
    "    \"\"\"Builds an in-memory inverted index of stemmed tokens (no stopwords).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.token_re = re.compile(r\"[a-z0-9]+\")\n",
    "        self.inverted = defaultdict(lambda: defaultdict(int))\n",
    "        self.inverted = defaultdict(postings_dict)\n",
    "        self.doc_meta = {}\n",
    "\n",
    "    def add_page(self, doc_id, url, raw_text):\n",
    "        # normalize → lowercase, tokenize, stem, filter stopwords\n",
    "        text = raw_text.lower()\n",
    "        tokens = self.token_re.findall(text)\n",
    "        for tok in tokens:\n",
    "            if tok in self.stopwords:\n",
    "                continue\n",
    "            stem = self.stemmer.stem(tok)\n",
    "            self.inverted[stem][doc_id] += 1\n",
    "        self.doc_meta[doc_id] = {\"url\": url}\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump((self.inverted, self.doc_meta), f)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            self.inverted, self.doc_meta = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " Example Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Example Domain \n",
      " This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission. \n",
      " More information... \n",
      " \n",
      " \n",
      " \n",
      " ...\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " Example Domains \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Domains \n",
      " Protocols \n",
      " Numbers \n",
      " About \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Example Domains \n",
      " As described in  RFC 2606  and  RFC 6761 , a\n",
      "number of domains such as example.com and example.org are maintained\n",
      "for documentation purposes. These domains may be used as illustrative\n",
      "examples in documents without prior coordination with us. They are not\n",
      "available for registration or transfer. \n",
      " We provide a web service on the example domain hosts to provide basic\n",
      "information on the purpose of the domain. These web services are\n",
      "provided as best effort, but are not designed to support production\n",
      "applications. While incidental traffic for incorrectly configured\n",
      "applications is expected, please do not design applications that require\n",
      "the example domains to have operating HTTP service. \n",
      " Further Reading \n",
      " \n",
      " IANA-managed Reserved Domains \n",
      " \n",
      " Last revised 2017-05-13. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Domain Names \n",
      " \n",
      " \n",
      " Root Zone Registry \n",
      " .INT Registry \n",
      " .ARPA Registry \n",
      " I ...\n",
      "Idx: 0 – https://example.com\n",
      "Idx: 1 – https://www.iana.org/domains/example\n",
      "Index saved to stem_index.pkl\n"
     ]
    }
   ],
   "source": [
    "seeds = [\"https://example.com\"]\n",
    "crawler = OfflineCrawler(seeds, max_depth=1)\n",
    "pages = crawler.run()\n",
    "\n",
    "index = StemIndex()\n",
    "for i, (url, text) in enumerate(pages):\n",
    "    print(f\"Idx: {i} – {url}\")\n",
    "    index.add_page(i, url, text)\n",
    "\n",
    "index.save(\"stem_index.pkl\")\n",
    "print(\"Index saved to stem_index.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
